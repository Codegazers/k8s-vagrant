
## DEMO_NGINX_Ingress_Controller_ActiveHealthChecks

asciinema rec -t "DEMO_NGINX_Ingress_Controller_ActiveHealthChecks"

# We deploy complete NGINX Plus Ingress Controller with a full deployment file

curl https://raw.githubusercontent.com/Codegazers/k8s-vagrant/master/demo/nginx-plus-ingress_full-deployment.yml | kubectl apply -f -


# We have created 'nginx-ingress' namespace so we will wait until all components are ready.

kubectl get all --namespace nginx-ingress


# We will deploy now applications with active health checks

curl https://raw.githubusercontent.com/Codegazers/k8s-vagrant/master/examples/colors/colors-with-health.yml | kubectl apply -f - 

# For quick access we prepare some environment variables

export INGRESS_IP=$(minikube   service -n nginx-ingress nginx-ingress --url --format "{{ .IP }}" | head -1)
export INGRESS_HTTP_PORT=$(minikube   service -n nginx-ingress nginx-ingress --url --format "{{ .Port }}" | head -1)
export INGRESS_HTTPS_PORT=$(minikube   service -n nginx-ingress nginx-ingress --url --format "{{ .Port }}" | tail -1)

# We review the deployments

kubectl get all --namespace default

kubectl get pods --namespace default


# Now we deploy colors-ingress ingress resource for accesing both applications

curl https://raw.githubusercontent.com/Codegazers/k8s-vagrant/master/examples/colors/colors-ingress-with-health.yml | kubectl apply -f -

# We can now review ingress resource

kubectl get ingress colors-ingress -o yaml


# And now we can access our applications setting host headers

http ${INGRESS_IP}:${INGRESS_HTTP_PORT}/text Host:red.example.com

http ${INGRESS_IP}:${INGRESS_HTTP_PORT}/text Host:blue.example.com

kubectl get pods --namespace default


# We can Review their health checks on /health path:

http ${INGRESS_IP}:${INGRESS_HTTP_PORT}/health Host:red.example.com

http ${INGRESS_IP}:${INGRESS_HTTP_PORT}/health Host:blue.example.com


# We can set one of them to failure state creating a file named "down" on /tmp

# For blue app we can use

kubectl get pods -l app=blue

# And create /tmp/down on latest pod ...

# We get latest pod
latest_blue_pod=$(kubectl get pods -o name -l app=blue|tail -1|cut -d "/" -f2)

# or
latest_blue_pod=$(kubectl get pods --selector=app=blue -o  jsonpath='{.items[:1].metadata.name}')
 

kubectl exec -ti ${latest_blue_pod} -- touch /tmp/down


# And now we can review status again 
# (at least a couple of times quickly because latest one it is going to be disabled)

http ${INGRESS_IP}:${INGRESS_HTTP_PORT}/health Host:blue.example.com

http ${INGRESS_IP}:${INGRESS_HTTP_PORT}/health Host:blue.example.com


# Review again after waiting some seconds and everything is healthy, why?

http ${INGRESS_IP}:${INGRESS_HTTP_PORT}/health Host:blue.example.com

http ${INGRESS_IP}:${INGRESS_HTTP_PORT}/health Host:blue.example.com


# Because in fact one of the backends has been disabled by Nginx
# And we always reach same backend

http ${INGRESS_IP}:${INGRESS_HTTP_PORT}/text Host:blue.example.com

http ${INGRESS_IP}:${INGRESS_HTTP_PORT}/text Host:blue.example.com

http ${INGRESS_IP}:${INGRESS_HTTP_PORT}/text Host:blue.example.com


# Now we set "alive" our "failed" backend

kubectl exec -ti ${latest_blue_pod} -- rm /tmp/down


# After some seconds, all backends will be available again

http ${INGRESS_IP}:${INGRESS_HTTP_PORT}/text Host:blue.example.com

http ${INGRESS_IP}:${INGRESS_HTTP_PORT}/text Host:blue.example.com

http ${INGRESS_IP}:${INGRESS_HTTP_PORT}/text Host:blue.example.com


# Recorded as asciinema
https://asciinema.org/a/aCZrXy4xQSxrBHH1m1imRGdCn



